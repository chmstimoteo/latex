\chapter{Swarm Intelligence Algorithms}
  \section{Particle Swarm Optimization}
    %\subsection{Agents}
    \subsection{Communications Topologies}
  \section{Adaptive Particle Swarm Optimization}
Zhan \emph{et al.} proposed the APSO in 2009, which updates the parameters of the PSO by using fuzzy rules. The APSO consists in a loop with the following steps: (\emph{i}) estimation and classification of the evolutionary state, (\emph{ii}) determination of the acceleration coefficients based on the evolutionary state, (\emph{iii}) diversity generation and (\emph{iv}) adaptation of the inertia factor.

The evolutionary state can be determined by evaluating the evolutionary factor ($f_{evol}$) as shown in Equation (\ref{eq:APSO_factor}).
\begin{equation}\label{eq:APSO_factor}
f_{evol} = \frac{d_g - d_{min}}{d_{max} - d_{min}} \ \  \in \ \ [0,1],
\end{equation}
where $d_g$ is the average distance between the best particle of the swarm and the rest of the swarm, $d_{min}$ and $d_{max}$ are the smallest and the biggest average distance among all particles, respectively. The average distance ($d_i$) of particle $i$ and rest of the swarm is evaluated by Equation (\ref{eq:APSO_distance}).
\begin{equation}\label{eq:APSO_distance}
d_i = \frac{1}{N-1}\sum_{j=1,j\neq i}^{N}\sqrt{\sum_{k=1}^{D}(x_i^k - x_j^k)},
\end{equation}
where $N$ is the number of particles in the swarm and $D$ is the number of dimensions.

The state of convergence of the swarm is classified based on fuzzy rules, where the evolutionary state is chosen at each iteration based on the membership function with the higher value. Figure presents the four membership functions used in the APSO. The next step is to update the acceleration coefficients $c_1$ and $c_2$, which are initialized with the values equal to 2.0 and are updated according to the current value of $f_{evol}$. The rules to update the coefficients are depicted in Table . We used the same step to update the coefficients presented in. The third step is the application of a operator in order to generate diversity. The authors named it be Learning Strategy using Elitism. This is a type of greedy local search applied only in one dimension of the current best particle of the swarm aiming to allow this particle to escape from a local optimum. The operator is a Gaussian mutation generated by a normal probability density function $N(0,\sigma)$, where $\sigma$ is called elitism learning rate. 
  \section{Artificial Bee Colony}
The ABC algorithm was proposed by Karaboga in 2005. ABC is modeled by the behavior of honey bees. The bee colony is one of the natural societies with the most specialized social divisions. In the simplified model assumed by the ABC, the bee colony is composed by three types of bees: employed bees, onlookers and scouts. The employed bees are those who go to the food source explored by herself. The bees that wait in the hive and decide to exploit a food source depending on the information shared by the employed bees are called onlooker bees. The onlookers are guided bees. The bees responsible for to find the new valuable food source are the scout bees and we will call them guide bees in exploration mode.

\subsection{Update of food source}

In the algorithm, the search space of the problem is $D$-dimensional. The number of the employed bees and the onlookers are the same and for every food source, there is only on employed bee. In other words, the size of employed and onlooker bees is equal $SN$ (the number of food sources). A food source represents a possible solution to the problem. Each $i$th source food associated to the employed bee will be optimized according to Equation (\ref{eq:ABC_employed}).
\begin{equation}\label{eq:ABC_employed}
v_{id} = x_{id} + r_{id}(x_{id} - x_{kd}),
\end{equation}

where $d = 1,2,...,D$ is number of dimensions, $r_{id}$ is a random generalized real number within the range [-1,1], $k = 1,2,...,SN$ is randomly selected index number in the colony, it has to be different from the $i$. The new solution ($v_{id}$) is compared with its original one ($x_{id}$), and the better one should be retained.

\subsection{Choose of food source by onlookers bees}
Next, the onlooker bee needs to select one of the food sources explored by the employed bees. The probability for each food source to be selected by the onlooker bee is:
\begin{equation}\label{eq:ABC_probability}
p_i = \frac{fit_i}{\sum_{j}^{SN}fit_j},
\end{equation}

where $p_i$ is the probability to select the food source $i$, which is proportional to the quality of the food source, $fit_i$ is the fitness of the solution $x_{id}$. Each onlooker bee searchers for a new solution in the food source selected by using Equation (\ref{eq:ABC_employed}).

\subsection{Stagnation of food source}
At each loop, the food source are evaluated and if the fitness of a source does not improve after a predetermined number of steps (called \textit{MaxTrial}), then it is abandoned. The employed bee associated with which becomes a scout and replaces the food source with new one, through the random search given by Equation (\ref{eq:ABC_scout}).
\begin{equation}\label{eq:ABC_scout}
x_{id} = x_d^{min} + r(x_d^{max} - x_d^{min}),
\end{equation}

where $r$ is random number selected in range [0,1], and $x_d^{max}$ and $x_d^{min}$ are lower and upper borders in the $d^{th}$ dimension of the search space.
  \section{Fish School Search}

  \section{Other Hybrids Approaches}
\pagebreak
